During Weeks 11 and 12, I integrated the newly trained emotion model into the music-generation workflow for the first time. This was the stage where I attempted to let the performerâ€™s facial emotion influence the selection of loops during playback. However, the results were not ideal.

In real-time tests, the transition between emotion segments was abrupt. The system switched loops too quickly, causing sudden changes that felt unnatural during performance. Although the model could detect emotion reliably, the musical output did not reflect emotional continuity. The generated tracks lacked a sense of structure and professionalism, sounding more like loosely stitched fragments rather than a cohesive techno composition.

This attempt made it clear that simply mapping emotions to loop changes was not enough. A more stable, musically aware structure would be necessary for real-time use.


https://git.arts.ac.uk/user-attachments/assets/bc508f7d-33a0-4739-8551-a69f92d50c16

