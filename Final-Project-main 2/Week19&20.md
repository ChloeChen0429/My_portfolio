During Weeks 19 and 20, I moved the seven emotion-based videos into TouchDesigner and began building the real-time visual system. My goal was to make the visuals respond both to emotion changes and to the audio generated by the music engine, so I first experimented with different TOP operators. I eventually chose Displace TOP as the main effect component. Displace allowed me to distort and shift the surface of the robot figure based on audio amplitude, creating a feeling of emotional “rupture” that visually matched the concept of unstable or transitioning affect.

<img width="640" alt="image" src="https://git.arts.ac.uk/user-attachments/assets/dc1e909d-0844-4957-9870-87cd5d9b0363" />

I mapped the mid-frequency band of the audio to the Displace weight, which produced a dynamic visual expansion–contraction effect. This made the robot appear as if she was splitting or vibrating under emotional influence. Conceptually, this distortion reflects two ideas: (1) emotion detection is imperfect and machine interpretation always introduces noise, and (2) the robot is both “born from” and “separated from” the performer’s emotional input.

https://git.arts.ac.uk/user-attachments/assets/a7209875-af5b-442c-a36c-7c1f08b4b14c

In the final step of this stage, I implemented OSC transmission between the music-generation script and TouchDesigner. The emotion model controlled the Switch TOP to select which of the seven videos should play, while the audio features controlled Displace-based motion. With OSC working in real time, the system began to function as a complete audiovisual loop, where sound and emotion jointly shaped the visual output.

https://git.arts.ac.uk/user-attachments/assets/1488bdfb-8a5a-492b-b722-1a1f638be887

