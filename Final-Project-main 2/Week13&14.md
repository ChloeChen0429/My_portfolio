During Weeks 13 and 14, I focused on improving the unstable results from my earlier attempts at emotion-driven music generation. The previous version suffered from abrupt emotional transitions and a lack of musical coherence. To address this, I began analysing the harmonic properties of all loops in my Berlin Underground dataset. I separated the loops with clear pitch from those without, extracted their keys, and grouped them by chord families. This allowed me to understand which loops could be musically combined, and which should remain purely percussive.

![image](https://git.arts.ac.uk/user-attachments/assets/0c354282-431f-438e-ac49-5374640806c9)

At the same time, I introduced structural learning from full techno tracks. I divided reference tracks into segments such as intro, build, mid, etc, and analysed the changes in energy and density across these sections. This process helped me build a simple rule-based structure that controlled when different types of loops should appear, preventing the system from sounding like random concatenation.

<img width="360" alt="image" src="https://git.arts.ac.uk/user-attachments/assets/544535a1-bd99-4454-85df-1642c4d31aac" />

![image](https://git.arts.ac.uk/user-attachments/assets/4f7c7522-21b2-4fd5-bfe5-7b34ddae5e3f)


By combining harmonic analysis with multi-section structural logic, I achieved the first version of facial-expression-to-music interaction that felt partially coherent. To smooth out the previously abrupt transitions between emotional states, I further implemented fade-in and fade-out effects when switching loops. This significantly reduced the harsh cuts and made the timing of emotional changes feel more natural in a performance context.

<img width="332" alt="image" src="https://git.arts.ac.uk/user-attachments/assets/0460a7a9-3f1e-4a29-9251-6ae06b358f30" />




https://git.arts.ac.uk/user-attachments/assets/dbbf6f75-4c35-4956-b534-d6119a9695d7

