During Weeks 15 and 16, I shifted my focus from the music system to the visual component. My initial plan was to generate emotion-driven visuals using ComfyUI. I hoped to build a workflow that could produce seamless loops and stylised robotic characters that matched the emotional output from my system. I experimented with several pipelines, including Diffusion models with IP-Adapter, frame-to-frame consistency nodes, and attempts to merge first-frame and last-frame latents to create continuous loops.

However, I quickly realised that this approach was not feasible for real-time performance. Even relatively lightweight workflows caused significant load on my computer, and rendering short video loops took far too long to be practical. More importantly, Diffusion-based workflows introduced noticeable latency and instability, making it impossible to combine with my real-time emotion-driven audio system. The computational demands were simply too heavy for live VJ-style interaction.

This stage made it clear that I needed a more responsive visual solution. As a result, I began shifting my plan away from full AI video generation in ComfyUI toward a system that could run smoothly inside TouchDesigner and react in real time to emotional input.


